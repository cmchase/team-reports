#!/usr/bin/env python3
"""
Shared GitHub API client for eliminating code duplication.

This module provides a centralized GitHub API client that can be shared
between weekly summaries, quarterly summaries, and lead time analysis.
"""

import os
import requests
import time
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Tuple
from dotenv import load_dotenv

from .config import get_config

# Load environment variables
load_dotenv()


class GitHubApiClient:
    """Centralized GitHub API client with common operations."""
    
    def __init__(self, config_file: str = 'config/github_config.yaml'):
        """Initialize the GitHub API client with configuration."""
        # Load GitHub token
        self.github_token = os.getenv('GITHUB_TOKEN')
        if not self.github_token:
            raise ValueError("GITHUB_TOKEN environment variable is required")
        
        # Load configuration
        self.config = get_config([config_file])
        
        # Extract commonly used config values
        self.repositories = self.config.get('repositories', [])
        self.github_org = self.config.get('github_org', '')
        self.base_url = 'https://api.github.com'
        self.headers = {
            'Authorization': f'token {self.github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }
    
    def _make_request(self, endpoint: str, params: Optional[Dict] = None) -> List[Dict]:
        """Make a request to GitHub API with pagination support."""
        url = f"{self.base_url}/{endpoint}"
        all_data = []
        page = 1
        
        while True:
            request_params = {'page': page, 'per_page': 100}
            if params:
                request_params.update(params)
            
            response = requests.get(url, headers=self.headers, params=request_params)
            response.raise_for_status()
            
            data = response.json()
            if not data:
                break
                
            all_data.extend(data)
            page += 1
            
            # GitHub API returns less than per_page if it's the last page
            if len(data) < 100:
                break
        
        return all_data
    
    def _fetch_pr_details(self, repo: str, pr_number: int) -> Dict:
        """Fetch detailed information for a specific pull request."""
        url = f"{self.base_url}/repos/{repo}/pulls/{pr_number}"
        
        # Add small delay to avoid hitting rate limits
        time.sleep(0.1)
        
        response = requests.get(url, headers=self.headers)
        response.raise_for_status()
        
        pr_data = response.json()
        return {
            'additions': pr_data.get('additions', 0),
            'deletions': pr_data.get('deletions', 0),
            'changed_files': pr_data.get('changed_files', 0)
        }
    
    def _fetch_pr_reviews(self, repo: str, pr_number: int) -> List[Dict]:
        """Fetch reviews for a specific pull request."""
        endpoint = f"repos/{repo}/pulls/{pr_number}/reviews"
        
        # Add small delay to avoid hitting rate limits
        time.sleep(0.1)
        
        try:
            return self._make_request(endpoint)
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Warning: Could not fetch reviews for PR #{pr_number}: {e}")
            return []
    
    def _fetch_pr_review_comments(self, repo: str, pr_number: int) -> List[Dict]:
        """Fetch review comments for a specific pull request."""
        endpoint = f"repos/{repo}/pulls/{pr_number}/comments"
        
        # Add small delay to avoid hitting rate limits
        time.sleep(0.1)
        
        try:
            return self._make_request(endpoint)
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Warning: Could not fetch review comments for PR #{pr_number}: {e}")
            return []
    
    def _build_repo_path(self, repo: str) -> str:
        """Build full repository path with organization if configured."""
        return f"{self.github_org}/{repo}" if self.github_org else repo
    
    def fetch_pull_requests(self, repo: str, start_date: str, end_date: str) -> List[Dict]:
        """Fetch pull requests for a repository in the date range with detailed information."""
        print(f"üîç Fetching pull requests for {repo} from {start_date} to {end_date}...")
        
        repo_path = self._build_repo_path(repo)
        endpoint = f"repos/{repo_path}/pulls"
        params = {
            'state': 'all',  # Get open, closed, and merged PRs
            'since': start_date,
            'sort': 'updated',
            'direction': 'desc'
        }
        
        prs = self._make_request(endpoint, params)
        
        # Filter by date range (GitHub API doesn't support date range filtering directly)
        filtered_prs = []
        for pr in prs:
            updated_at = datetime.fromisoformat(pr['updated_at'].replace('Z', '+00:00'))
            start_dt = datetime.fromisoformat(f"{start_date}T00:00:00+00:00")
            end_dt = datetime.fromisoformat(f"{end_date}T23:59:59+00:00")
            
            if start_dt <= updated_at <= end_dt:
                # Fetch detailed PR info for code metrics
                try:
                    detailed_pr = self._fetch_pr_details(repo_path, pr['number'])
                    pr.update(detailed_pr)
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  Warning: Could not fetch details for PR #{pr['number']}: {e}")
                    pr['additions'] = 0
                    pr['deletions'] = 0
                    pr['changed_files'] = 0
                
                # Fetch reviews and review comments if review_depth is enabled
                if self.config.get('metrics', {}).get('delivery', {}).get('review_depth', False):
                    pr['reviews'] = self._fetch_pr_reviews(repo_path, pr['number'])
                    pr['review_comments'] = self._fetch_pr_review_comments(repo_path, pr['number'])
                else:
                    pr['reviews'] = []
                    pr['review_comments'] = []
                
                filtered_prs.append(pr)
        
        return filtered_prs
    
    def fetch_commits(self, repo: str, start_date: str, end_date: str) -> List[Dict]:
        """Fetch commits for a repository in the date range."""
        print(f"üîç Fetching commits for {repo} from {start_date} to {end_date}...")
        
        repo_path = self._build_repo_path(repo)
        endpoint = f"repos/{repo_path}/commits"
        params = {
            'since': f"{start_date}T00:00:00Z",
            'until': f"{end_date}T23:59:59Z"
        }
        
        return self._make_request(endpoint, params)
    
    def fetch_issues(self, repo: str, start_date: str, end_date: str) -> List[Dict]:
        """Fetch issues for a repository in the date range."""
        print(f"üîç Fetching issues for {repo} from {start_date} to {end_date}...")
        
        repo_path = self._build_repo_path(repo)
        endpoint = f"repos/{repo_path}/issues"
        params = {
            'state': 'all',
            'since': f"{start_date}T00:00:00Z",
            'sort': 'updated',
            'direction': 'desc'
        }
        
        issues = self._make_request(endpoint, params)
        
        # Filter out pull requests (GitHub API includes PRs in issues) and by date range
        filtered_issues = []
        for issue in issues:
            if 'pull_request' not in issue:
                updated_at = datetime.fromisoformat(issue['updated_at'].replace('Z', '+00:00'))
                start_dt = datetime.fromisoformat(f"{start_date}T00:00:00+00:00")
                end_dt = datetime.fromisoformat(f"{end_date}T23:59:59+00:00")
                
                if start_dt <= updated_at <= end_dt:
                    filtered_issues.append(issue)
        
        return filtered_issues
    
    def fetch_all_data(self, start_date: str, end_date: str) -> Dict[str, Dict[str, List[Dict]]]:
        """Fetch all GitHub data for the specified date range."""
        all_data = {
            'pull_requests': {},
            'commits': {},
            'issues': {}
        }
        
        print(f"üöÄ Fetching GitHub data for {len(self.repositories)} repositories...")
        
        for repo in self.repositories:
            repo_path = self._build_repo_path(repo)
            print(f"\nüìÅ Processing repository: {repo}")
            
            try:
                all_data['pull_requests'][repo_path] = self.fetch_pull_requests(repo, start_date, end_date)
                all_data['commits'][repo_path] = self.fetch_commits(repo, start_date, end_date)
                all_data['issues'][repo_path] = self.fetch_issues(repo, start_date, end_date)
                
                pr_count = len(all_data['pull_requests'][repo_path])
                commit_count = len(all_data['commits'][repo_path])
                issue_count = len(all_data['issues'][repo_path])
                
                print(f"  ‚úÖ Found: {pr_count} PRs, {commit_count} commits, {issue_count} issues")
                
            except Exception as e:
                print(f"  ‚ùå Error fetching data for {repo}: {e}")
                all_data['pull_requests'][repo_path] = []
                all_data['commits'][repo_path] = []
                all_data['issues'][repo_path] = []
        
        return all_data
    
    def get_merged_prs_for_lead_time(self, start_date: str, end_date: str, 
                                   all_prs_data: Optional[Dict[str, List[Dict]]] = None) -> List[Dict]:
        """
        Get merged PRs for lead time analysis, using pre-fetched data if available.
        
        Args:
            start_date: Start date in YYYY-MM-DD format
            end_date: End date in YYYY-MM-DD format  
            all_prs_data: Optional pre-fetched PR data to avoid duplicate API calls
            
        Returns:
            List of merged PRs with detailed information
        """
        if all_prs_data:
            print("‚úÖ Using pre-fetched PR data (optimized)")
            # Collect all merged PRs from pre-fetched data
            all_prs = []
            for repo_path, prs in all_prs_data.items():
                for pr in prs:
                    if pr.get('merged_at'):
                        merged_date = pr['merged_at'][:10]  # Extract YYYY-MM-DD
                        if start_date <= merged_date <= end_date:
                            all_prs.append(pr)
            return all_prs
        else:
            print("‚ö†Ô∏è  Making fresh API calls (not optimized)")
            # Fallback: reuse existing fetch_all_data method to avoid duplication
            all_data = self.fetch_all_data(start_date, end_date)
            
            # Extract merged PRs from the fetched data
            all_prs = []
            for repo_path, prs in all_data['pull_requests'].items():
                for pr in prs:
                    if pr.get('merged_at'):
                        merged_date = pr['merged_at'][:10]  # Extract YYYY-MM-DD
                        if start_date <= merged_date <= end_date:
                            all_prs.append(pr)
            
            return all_prs
